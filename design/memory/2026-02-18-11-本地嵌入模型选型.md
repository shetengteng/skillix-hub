# 方案 C：本地嵌入模型选型

> **版本**: v1.0
> **创建日期**: 2026-02-18
> **系列**: 方案 C 设计文档 ([返回总览](./2026-02-18-01-总览与设计理念.md))

---

## 1. 为什么需要嵌入模型

记忆系统中，SQLite 索引支持两种搜索方式：

| 搜索方式 | 依赖 | 能力 |
|----------|------|------|
| **FTS5 关键词搜索** | 无（SQLite 内置） | 精确匹配关键词，BM25 排名 |
| **向量相似度搜索** | 嵌入模型 | 语义匹配（"数据库" 能匹配到 "PostgreSQL"） |

FTS5 是基础能力，不需要嵌入模型即可工作。但当用户使用不同措辞搜索同一概念时（如搜索"缓存方案"能找到"Redis 存储"相关记忆），需要向量搜索的语义理解能力。

**核心要求**：使用本地模型，不依赖任何外部 API。

---

## 2. BGE 模型背景

**BGE（BAAI General Embedding）** 由 **BAAI（北京智源人工智能研究院，Beijing Academy of Artificial Intelligence）** 开发和维护。

| 项目 | 说明 |
|------|------|
| **机构名称** | 北京智源人工智能研究院（BAAI） |
| **成立时间** | 2018 年，由北京市政府支持成立 |
| **性质** | 非营利性 AI 研究机构 |
| **代表成果** | BGE 嵌入模型系列、FlagEmbedding 训练框架、悟道大模型系列 |
| **开源协议** | MIT（BGE 系列均为 MIT 许可，可商用） |
| **模型仓库** | [huggingface.co/BAAI](https://huggingface.co/BAAI) |
| **行业地位** | MTEB（英文嵌入基准）和 C-MTEB（中文嵌入基准）榜单长期前列 |

BGE 系列是目前中文嵌入领域最广泛使用的开源模型家族，从小型（24M 参数）到多语言大型（560M 参数）覆盖各种场景。其 `FlagEmbedding` 框架支持模型微调，用户可以在自己的数据上进一步优化检索效果。

---

## 3. 候选模型对比

### 3.1 对比总览

由于记忆系统中存储的内容为中英文混合（中文对话 + 英文技术术语/代码），候选模型按中英文支持能力和模型规模排列：

| 模型 | 出品方 | 参数量 | 向量维度 | 模型大小 | 中文 | 英文 | C-MTEB | 许可 |
|------|--------|--------|----------|----------|------|------|--------|------|
| **Qwen3-Embedding-0.6B** | 阿里通义千问 | 600M | 1024 | ~1.2GB | ★★★★★ | ★★★★★ | 71.02 | Apache 2.0 |
| **BGE-M3** | BAAI 智源 | 560M | 1024 | ~2.2GB | ★★★★★ | ★★★★★ | 63.0 | MIT |
| **BGE-large-zh-v1.5** | BAAI 智源 | 326M | 1024 | ~1.3GB | ★★★★★ | ★★★ | — | MIT |
| **BGE-base-zh-v1.5** | BAAI 智源 | 102M | 768 | ~410MB | ★★★★★ | ★★★ | — | MIT |
| **BGE-small-zh-v1.5** | BAAI 智源 | 24M | 512 | ~96MB | ★★★★ | ★★☆ | — | MIT |
| **BGE-base-en-v1.5** | BAAI 智源 | 110M | 768 | ~440MB | ★★ | ★★★★★ | — | MIT |
| **all-MiniLM-L6-v2** | Sentence-Transformers | 22M | 384 | ~80MB | ★ | ★★★★ | — | Apache 2.0 |
| **GTE-small** | 阿里达摩院 | 33M | 384 | ~130MB | ★★ | ★★★★ | — | MIT |

> **注意**：智谱（Zhipu/GLM）仅提供 API 嵌入模型（embedding-2/3），**无开源本地模型**，不符合本方案"零外部依赖"的要求，因此未纳入对比。

### 3.2 各模型详解

#### Qwen3-Embedding-0.6B（阿里通义千问）— 新一代最佳性价比

```
来源: Qwen / Hugging Face & ModelScope
发布: 2025 年 6 月
参数: 600M（基于 Qwen3 基座模型）
维度: 1024（支持 MRL 灵活降维）
大小: ~1.2GB
许可: Apache 2.0
支持语言: 100+（含代码语言）
最大输入: 32,768 tokens
```

**优势**：
- **C-MTEB 71.02**，远超 BGE-M3（63.0），中英文效果均为当前最佳水平
- 支持 **Matryoshka Representation Learning（MRL）**：可将 1024 维降至 256/512 等自定义维度，节省存储
- 32K 上下文长度，远超 BGE-M3 的 8K，适合长文本会话摘要
- 支持**指令感知**：为不同任务定制提示词可额外提升 1-5% 性能
- Apache 2.0 许可，可商用
- Qwen 系列三个规格（0.6B / 4B / 8B），可按需升级

**劣势**：
- 模型较新（2025 年 6 月发布），社区使用时间不如 BGE 长
- 600M 参数比 BGE-small-zh（24M）大很多，不算"极致轻量"
- 依赖 `transformers` 库加载，不如 `sentence-transformers` 方便

**适用**：追求最佳中英双语质量的场景（强烈推荐）

#### BGE-M3（BAAI 智源）— 成熟稳定的多语言方案

```
来源: BAAI / Hugging Face
参数: 560M（基于 XLM-RoBERTa）
维度: 1024
大小: ~2.2GB
许可: MIT
支持语言: 100+（中英文效果优秀）
最大输入: 8192 tokens
```

**优势**：
- 社区广泛验证，生态成熟
- 支持稠密向量 + 稀疏向量 + ColBERT 三种检索方式
- `sentence-transformers` 原生支持，集成简单

**劣势**：
- C-MTEB 分数（63.0）低于 Qwen3-Embedding-0.6B（71.02）
- 模型文件更大（2.2GB vs 1.2GB）
- 上下文长度更短（8K vs 32K）

**适用**：追求稳定性和成熟生态的场景

#### BGE-base-zh-v1.5（BAAI 智源）— 中文平衡方案

```
来源: BAAI / Hugging Face
参数: 102M
维度: 768
大小: ~410MB
许可: MIT
支持语言: 中文（英文可接受）
```

**优势**：
- 模型适中（410MB），加载速度可接受（3-5 秒）
- 768 维向量提供较丰富的语义表达

**劣势**：
- 英文语义理解不如多语言模型
- 跨语言搜索能力有限

**适用**：主要使用中文、英文仅为技术术语的场景

#### BGE-small-zh-v1.5（BAAI 智源）— 极致轻量

```
来源: BAAI / Hugging Face
参数: 24M
维度: 512
大小: ~96MB
许可: MIT
支持语言: 中文
```

**优势**：
- 最小的中文模型，加载极快（~1 秒）
- 内存占用极低，适合快速开发验证

**劣势**：
- 语义理解能力不如 base/large 版本
- 英文支持较弱

**适用**：**开发验证阶段首选**（快速迭代，后续可平滑切换到更强模型）

#### 纯英文模型（对比参考）

| 模型 | 出品方 | 参数 | 维度 | 大小 | 说明 |
|------|--------|------|------|------|------|
| BGE-base-en-v1.5 | BAAI 智源 | 110M | 768 | 440MB | 英文最佳平衡方案 |
| all-MiniLM-L6-v2 | Sentence-Transformers | 22M | 384 | 80MB | 最轻量，仅英文场景 |
| GTE-small | 阿里达摩院 | 33M | 384 | 130MB | 英文轻量替代 |

这些模型**不推荐**用于中英混合场景，仅供纯英文项目参考。

#### 不可用模型说明

| 模型 | 出品方 | 原因 |
|------|--------|------|
| Zhipu embedding-2/3 | 智谱清言 | 仅 API 服务，无开源本地模型 |
| Qwen3-Embedding-4B/8B | 阿里通义千问 | 可本地部署但模型过大（4B/8B 参数），不适合 Hook 脚本场景 |

---

## 4. 推荐方案

### 4.1 开发阶段默认：BGE-small-zh-v1.5

开发验证阶段使用最轻量的模型，确保快速迭代：

| 维度 | 值 |
|------|-----|
| 模型 | `BAAI/bge-small-zh-v1.5` |
| 向量维度 | 512 |
| 模型大小 | ~96MB |
| 每条记忆存储开销 | 2048 字节（512 × 4 bytes） |
| 1000 条记忆的向量总大小 | ~2MB |
| 首次加载时间 | ~1 秒 |
| 单条文本嵌入耗时 | ~3-5ms（CPU） |

**选择理由**：
1. 96MB 极轻量，下载和加载都很快，适合开发调试
2. 中文效果已足够验证系统功能
3. 与后续模型（base-zh / M3 / Qwen3）接口一致，可无缝切换
4. 不阻塞 sessionStart 的 10 秒超时

### 4.2 生产推荐：Qwen3-Embedding-0.6B

验证完成后，推荐切换到 Qwen3-Embedding-0.6B 作为生产模型：

| 维度 | BGE-small-zh-v1.5（开发） | Qwen3-Embedding-0.6B（生产） |
|------|--------------------------|------------------------------|
| 中文效果 | 良好 | **最佳**（C-MTEB 71.02） |
| 英文效果 | 弱 | **优秀** |
| 跨语言 | 弱 | **强**（100+ 语言） |
| 模型大小 | 96MB | 1.2GB |
| 向量维度 | 512 | 1024（可 MRL 降至 256/512） |
| 首次加载 | ~1s | ~3-5s |
| 最大输入 | 512 tokens | **32K tokens** |

**Qwen3 的关键优势**：
1. C-MTEB 71.02，远超 BGE-M3（63.0），是当前开源中文嵌入模型的最高水平
2. 支持 MRL（Matryoshka Representation Learning），可灵活选择 256/512/1024 维
3. 32K 上下文，适合嵌入较长的会话摘要
4. Apache 2.0 许可，可商用

### 4.3 备选：BGE-M3

如果 Qwen3-Embedding-0.6B 在实际使用中不稳定，可回退到 BGE-M3：

| 维度 | Qwen3-Embedding-0.6B | BGE-M3 |
|------|----------------------|--------|
| C-MTEB | 71.02 | 63.0 |
| 模型大小 | 1.2GB | 2.2GB |
| 生态成熟度 | 较新（2025.06） | 成熟 |
| sentence-transformers 支持 | 需适配 | 原生支持 |

### 4.4 降级：无嵌入模型

如果嵌入模型不可用（如用户未安装依赖），系统自动降级为纯 FTS5 关键词搜索。功能不受影响，仅语义搜索能力缺失。

### 4.5 推荐决策树

```
开发验证阶段
└── BGE-small-zh-v1.5（96MB，极致轻量）

生产部署阶段
├── 中英双语需求？
│   ├── 是 → Qwen3-Embedding-0.6B（1.2GB，最佳效果，推荐）
│   │        └── 不稳定？→ BGE-M3（2.2GB，成熟方案）
│   └── 否（纯中文）→ BGE-base-zh-v1.5（410MB，中文平衡）
│
└── 资源极度受限？→ BGE-small-zh-v1.5（96MB，继续使用）

任何阶段，无依赖时 → 自动降级为纯 FTS5 关键词搜索
```

---

## 5. 技术实现

### 5.1 Python 依赖

```bash
pip install sentence-transformers
```

`sentence-transformers` 会自动安装 `torch`（CPU 版本即可）和 `transformers`。

### 5.2 嵌入生成代码

开发阶段使用 BGE-small-zh-v1.5（通过 sentence-transformers 加载）：

```python
from sentence_transformers import SentenceTransformer
import struct

DEFAULT_MODEL = 'BAAI/bge-small-zh-v1.5'
_model = None

def get_model(model_name: str = None):
    global _model
    if _model is None:
        _model = SentenceTransformer(model_name or DEFAULT_MODEL)
    return _model

def embed_text(text: str) -> bytes:
    model = get_model()
    embedding = model.encode(text, normalize_embeddings=True)
    return struct.pack(f'{len(embedding)}f', *embedding)

def deserialize_embedding(blob: bytes) -> list[float]:
    n = len(blob) // 4
    return list(struct.unpack(f'{n}f', blob))
```

切换到 Qwen3-Embedding-0.6B 时仅需修改 `DEFAULT_MODEL`：

```python
DEFAULT_MODEL = 'Qwen/Qwen3-Embedding-0.6B'
```

### 5.3 模型缓存

模型缓存在全局目录 `~/.memory/models/`。首次使用时通过 `init.py` 自动下载，后续直接从缓存加载。即使删除 skill 代码，模型缓存仍然保留，避免重复下载。

| 事件 | 耗时 |
|------|------|
| 首次下载（BGE-small） | 取决于网络（~130MB） |
| 从缓存加载 | 2-3 秒 |
| 单条文本嵌入 | 5-10ms（CPU） |
| 批量 100 条嵌入 | 200-500ms（CPU） |

### 5.4 模型配置

嵌入模型名称存储在 SQLite 的 `meta` 表中：

```sql
-- 开发阶段
INSERT INTO meta (key, value) VALUES ('embedding_model', 'BAAI/bge-small-zh-v1.5');
INSERT INTO meta (key, value) VALUES ('embedding_dims', '512');

-- 生产阶段（切换后）
-- UPDATE meta SET value = 'Qwen/Qwen3-Embedding-0.6B' WHERE key = 'embedding_model';
-- UPDATE meta SET value = '1024' WHERE key = 'embedding_dims';
```

如果用户切换模型（维度变化），需要清空 `chunks` 表中的 `embedding` 列并重新生成。

---

## 6. 性能预估

### 6.1 BGE-small-zh-v1.5（开发阶段默认）

在 sessionStart Hook 中（10s 超时）：

| 操作 | 耗时 | 说明 |
|------|------|------|
| 加载模型（从缓存） | ~1s | 96MB 极小 |
| 增量同步 10 条新记录 | ~100ms | 读取 JSONL + 写入 SQLite |
| 为 10 条新记录生成嵌入 | ~40ms | CPU 批量推理 |
| 读取 MEMORY.md + facts | ~50ms | 文件 I/O |
| 搜索 SQLite（FTS5 + 向量） | ~20ms | 查询 + 排序 |
| **总计** | **~1.2s** | 远低于 10s 超时 |

### 6.2 Qwen3-Embedding-0.6B（生产推荐）

在 sessionStart Hook 中（10s 超时）：

| 操作 | 耗时 | 说明 |
|------|------|------|
| 加载模型（从缓存） | ~3-5s | 已预下载的情况 |
| 增量同步 + 嵌入 10 条 | ~300ms | 读取 + 推理 + 写入 |
| 读取 + 搜索 | ~80ms | 文件 I/O + SQLite 查询 |
| **总计** | **~3.5-5.5s** | 在 10s 超时内完成 |

### 6.3 BGE-M3（备选）

在 sessionStart Hook 中（建议超时 20s）：

| 操作 | 耗时 | 说明 |
|------|------|------|
| 加载模型（从缓存） | ~3-5s | 已预下载 |
| 增量同步 + 嵌入 10 条 | ~400ms | 较大模型推理稍慢 |
| 读取 + 搜索 | ~80ms | 文件 I/O + SQLite |
| **总计** | **~4-6s** | 在 20s 超时内完成 |

---

## 相关文档

- [01-总览与设计理念](./2026-02-18-01-总览与设计理念.md) — 方案概述
- [07-JSONL与SQLite关系](./2026-02-18-07-JSONL与SQLite关系.md) — 嵌入向量在 SQLite 中的存储
- [08-SQLite索引设计](./2026-02-18-08-SQLite索引设计.md) — 向量搜索实现
