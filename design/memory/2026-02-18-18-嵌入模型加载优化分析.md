# 嵌入模型加载优化分析

## 问题

当前日志显示：
```
[memory.embedding] 模型加载完成 dim=512 (耗时 7.0s)
```

首次加载 BGE-small-zh-v1.5（96MB）需要约 7 秒，影响搜索和同步操作的首次响应速度。

## 耗时分析

7 秒的耗时由以下部分构成：

| 阶段 | 耗时 | 说明 |
|------|------|------|
| `import sentence_transformers` | ~2-3s | 导入 PyTorch + Transformers 库 |
| 模型文件读取 | ~0.5s | 从磁盘读取 96MB 模型权重 |
| 模型初始化 | ~1-2s | 构建计算图、加载到内存 |
| Tokenizer 初始化 | ~0.5-1s | 加载分词器 |

**核心瓶颈不是模型本身，而是 Python 库的冷启动（import）。**

## 可选优化方案

### 方案 A：常驻进程（后台 Server）

**原理**：启动一个 HTTP/Unix Socket 服务，模型常驻内存，脚本通过 API 调用。

```
init.py → 启动 embedding_server.py (后台)
search_memory.py → curl http://localhost:PORT/embed
sync_index.py → curl http://localhost:PORT/embed_batch
```

- 优点：首次加载后，后续调用 <50ms
- 缺点：需要管理进程生命周期、占用内存（~200MB）、增加复杂度
- **适用场景**：频繁搜索、大量同步

### 方案 B：预热脚本（Warm-up at sessionStart）

**原理**：在 `load_memory.py`（sessionStart Hook）执行时，预热模型到缓存。

```python
# load_memory.py 末尾
import threading
def _preload():
    from lib.embedding import is_available
    is_available()  # 触发模型加载
threading.Thread(target=_preload, daemon=True).start()
```

- 优点：零改动成本，会话开始时后台加载，等用户真正搜索时模型已就绪
- 缺点：第一次 sessionStart 仍然多花 ~7s（但因为是后台线程，不阻塞 Hook 输出）
- **适用场景**：当前阶段最合适

### 方案 C：ONNX Runtime 加速

**原理**：将 PyTorch 模型转换为 ONNX 格式，使用轻量级 ONNX Runtime 推理。

```
sentence-transformers (PyTorch) → 冷启动 ~7s
onnxruntime + 导出模型         → 冷启动 ~1-2s
```

- 优点：无需 PyTorch，启动快 3-5 倍，推理也快
- 缺点：需要额外的模型转换步骤、ONNX Runtime 兼容性
- **适用场景**：追求极致性能时

### 方案 D：延迟加载 + 缓存（当前已有）

**现状**：`embedding.py` 已实现单例缓存，首次调用加载，后续复用。

```python
_model = None  # 全局单例
def _load_model():
    global _model
    if _model is not None:
        return _model  # 后续调用 <1ms
    _model = SentenceTransformer(...)  # 首次 ~7s
```

- 优点：简单、已实现
- 缺点：每次 Python 进程启动都要重新加载（脚本是独立进程）
- **这是问题的本质**：每个脚本调用 = 新进程 = 重新 import + 加载模型

## 推荐方案

### 当前阶段：方案 B（预热）

成本最低、效果明显：

1. `sessionStart` 时后台线程预加载模型
2. 当用户在会话中触发搜索时，模型已经在进程缓存中...
3. **但注意**：Cursor 的 Hook 是独立进程，sessionStart 的进程退出后缓存就丢失了

因此方案 B 对"脚本独立进程"场景无效。**修正方案：**

### 实际推荐：方案 D（当前已有）+ 接受首次延迟

**理由：**
1. Memory Skill 的调用频率低（sessionStart 1 次，搜索偶尔几次）
2. 7 秒延迟只发生在首次调用，同一进程内后续调用 <1ms
3. `sync_index.py` 是唯一可能批量调用 embedding 的场景，它本身就是一个长进程
4. `search_memory.py` 每次搜索独立进程，7 秒冷启动不可避免——但搜索本身不是高频操作

### 未来优化路线

如果需要进一步优化，按优先级：

1. **ONNX Runtime 替代**（方案 C）— 降低冷启动到 1-2 秒
2. **常驻服务**（方案 A）— 适合搜索频繁的场景
3. **预计算缓存**（补充方案）— sync_index 时预计算所有向量，搜索时只需计算 query 向量

## 结论

当前 7 秒延迟主要来自 Python/PyTorch 库冷启动，而非模型本身。在"每次脚本调用 = 新进程"的 Cursor Hook 架构下，缓存无法跨进程复用。建议**接受当前延迟**，在日志中明确标识首次加载，让用户感知到原因。未来如需优化，优先考虑 ONNX Runtime 方案。
